<!DOCTYPE html>
<html lang="en">

  <head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>CS5785 Final Exam</title>

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/clean-blog.min.css" rel="stylesheet">

    <!-- Custom Fonts -->
<!--     <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'> -->

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

  </head>

  <body>

    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-custom navbar-fixed-top">
      <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="http://tech.cornell.edu/">Cornell Tech</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
          <ul class="nav navbar-nav navbar-right">
            <li>
              <a href="index.html">Home</a>
            </li>
            <li>
              <a href="lectures.html">Lectures</a>
            </li>
            <li>
              <a href="assignments.html">Assignments</a>
            </li>
            <li>
              <a href="contact.html">Contact</a>
            </li>
          </ul>
        </div>
        <!-- /.navbar-collapse -->
      </div>
      <!-- /.container -->
    </nav>

    <!-- Page Header -->
    <!-- Set your background image for this header on the line below. -->
    <header class="intro-header" style="background-image: url('img/banner.png'); background-color: #777;">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
            <div class="page-heading">
              <h1>Final Exam</h1>
              <hr class="small">
              <span class="subheading">Instructions</span>
            </div>
          </div>
        </div>
      </div>
    </header>

    <!-- Main Content -->
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">

          <br /><br /><br /><br />

          <h1>Please see the exam page on <a href="https://inclass.kaggle.com/c/cs5785-spring-2017-final">Kaggle</a>!</h1>

          <br /><br /><br /><br />
          <br /><br /><br /><br />

          <div>
            <h1>About The Exam</h1>
            <p>The format of the exam is a mock peer-reviewed conference. You will develop an algorithm, prepare a professional paper, submit an anonymized version to&nbsp;the EasyChair conference system, and peer-review the work from other groups. There are three deliverables:</p>
            <ul>
              <li><strong>Phase 1: Develop your algorithm and write your writeup</strong></li>
              <ul>
                <li>Develop your algorithm and submit your results to the Kaggle leaderboard.&nbsp;<strong>Deadline:&nbsp;Monday,&nbsp;May 15, 11:59 PM EST.</strong></li>
                <li>Write your paper submission and submit to CMS.&nbsp;<strong>Deadline:&nbsp;Monday,&nbsp;May 15, 11:59 PM EST</strong>. There should be only one submission from each team.</li>
              </ul>
              <li><strong>Phase 2: Peer-review other groups' work!</strong></li>
              <ul>
                <li>Write reviews and submit them to CMS.&nbsp;<strong>Deadline:&nbsp;Tuesday, May 16, 11:59 PM EST</strong>. (Will open after Phase 1 ends)</li>
                <li><em>Everybody</em>&nbsp;on each team must complete peer reviews individually. (They are not "per-team")</li>
              </ul>
            </ul>
            <h1>The Challenge: Build a large-scale image search engine!</h1>
            <p>You and your team of&nbsp;<strong>two Cornell Tech students</strong>&nbsp;are surely on the path to fame and fortune! You have been recruited by Google to disrupt Google Image Search by building a better search engine using novel statistical learning techniques.</p>
            <p>The specifications are simple: We need a way to&nbsp;<strong>search for relevant images</strong>&nbsp;given a natural language query. For instance, if a user types "dog jumping to catch frisbee," your system will&nbsp;<strong>rank-order the most relevant images</strong>&nbsp;from a large database.</p>
            <p>Here are some details:</p>
            <ul>
              <li><strong>During training,</strong>&nbsp;you have a dataset of 10,000 samples. Each sample has the following data available for learning:</li>
              <ul>
                <li>A 224x224 JPG image.</li>
                <li>A list of tags indicating objects appeared in the image.</li>
                <li>Feature vectors extracted using&nbsp;<a href="https://arxiv.org/abs/1512.03385">ResNet</a>, a state-of-the-art Deep-learned CNN (You don't have to train or run ResNet -- we are providing the features for you). See&nbsp;<a href="http://ethereon.github.io/netscope/#/gist/b21e2aae116dc1ac7b50">here</a>&nbsp;for the illustration of the ResNet-101 architecture. The features are extracted from&nbsp;<em>pool5</em>&nbsp;and&nbsp;<em>fc1000</em>&nbsp;layer.</li>
                <li>A five-sentence description, used to train your search engine.</li>
              </ul>
              <li><strong>During testing,</strong>&nbsp;your system matches a&nbsp;<em>single five-sentence description</em>&nbsp;against a pool of 2,000&nbsp;<em>candidate samples</em>&nbsp;from the test set. Each sample has:</li>
              <ul>
                <li>A 224x224 JPEG image.</li>
                <li>A list of tags for that image.</li>
                <li>ResNet feature vectors for that image.</li>
              </ul>
            </ul>
            <ul>
              <li><strong>Output</strong>: For each description, your system must&nbsp;<strong>rank-score</strong>&nbsp;each testing image with the likelihood of that image matches the given sentence. Your system then returns the name&nbsp;of the top 20 relevant images, delimited by space. See "sample_submission.csv" on the data page&nbsp;for&nbsp;more details on&nbsp;the output format.</li>
              <li><strong>Evaluation metric</strong>: There are 2,000 descriptions, and for each description, you must compare against the entire 2,000-image test set. That is, rank-order test images for each test description. We will use&nbsp;<strong>MAP@20</strong>&nbsp;as the evaluation metric. If the corresponding image of a description is among your algorithm's 20 highest scoring images, this metric gives you&nbsp;a&nbsp;certain score based on the ranking of the corresponding image. Please refer to&nbsp;the evaluation page for more details.&nbsp;</li>
            </ul>
            <p>Use all of your skills, tools, and experience. It is OK to use libraries like numpy, scikit-learn, pandas, etc., as long as you cite them. Use cross-validation on training set to debug your algorithm.&nbsp;Submit your results to the Kaggle leaderboard and send your complete writeup to EasyChair. The data you use --- and the way you use the data --- is completely up to you.</p>
            <p>The best teams (of&nbsp;<strong>two Cornell Tech students</strong>, recall) might use visualization techniques for debugging (e.g., show top images retrieved by your algorithm and see whether they make sense or not), preprocessing, a nice way to compare tags and descriptions, leveraging visual features and combining them with tags and descriptions, supervised and/or unsupervised learning to best understand how to best take advantage of each data source available to them.</p>
            <p>The best reports will be&nbsp;professionally written. We suggest including&nbsp;an&nbsp;<strong>Abstract,</strong>&nbsp;followed by an&nbsp;<strong>Introduction</strong>&nbsp;section including motivations of your method and a detailed description of each step in your pipeline / framework you used for this task. Then, include&nbsp;a detailed&nbsp;<strong>Experiment</strong>&nbsp;section and a&nbsp;<strong>Results</strong>&nbsp;section. If time permits, a&nbsp;<strong>Background / Related Work</strong>&nbsp;section can help to point out the relevant work&nbsp;in the image retrieval literature.</p>
            <p>The best reports use a professional style adopted by a major academic conference. NIPS is a good choice. Teams can download a template from Section 4 Paper Format of the&nbsp;<a href="https://nips.cc/Conferences/2015/PaperInformation/AuthorSubmissionInstructions">NIPS Author Guidelines</a>. We recommend you to use the LaTeX template, but this is not a requirement. If you're more familiar with Word, use Word.</p>
            <p>The best peer reviews will be detailed and thorough, pointing out potential areas of improvement as well as complimenting their peers' strengths.</p>
            <p>The best teams understand how to divide the precious time and energy among all members of the group. They might split into roles, preventing programming students from messing up the writing and ensuring writing gurus don't screw up the code. Close contact is always maintained within top-performing teams. When hardship happens within the best teams, blame is not assigned---it is overcome.</p>
            <p>May the best team win!</p>
            <h2>Rules</h2>
            <p>You are strongly encouraged to work in a group of <strong>two</strong> students.</p>
            <p>Groups may not collaborate with other groups. You cannot cite other teams' work. This includes sharing data, intermediate and final results, writing, or discussing your detailed method with other groups. You&nbsp;<strong>may not</strong>&nbsp;view reports from other teams until&nbsp;<strong>after</strong>&nbsp;the peer-review phase begins!</p>
            <h1>How to anonymize your&nbsp;paper</h1>
            <p>The "Peer Review" phase will be double-blind. This means authors will not know who the reviewers are, and the reviewers should not know who the authors are. This protects both sides.</p>
            <p>To maintain anonymity,&nbsp;<strong>don't</strong>&nbsp;include team member names in the actual paper that you submit to Dropbox!</p>
            <p>However, we need to know who you are. So please use the following file name:
            <code>TeamNameOnKaggle_Author1lastname_Author2lastname_report.pdf</code>
            This will be hidden from your reviewers.</p>
            <p><img src="http://i.imgur.com/F255rFT.png" alt="" width="368" height="309"></p>
            <h2>Peer Review Tips</h2>
            <p>See the following resources for tips on how to write a professional review:</p>
            <ul>
              <li><span>Matthew Might’s notes on&nbsp;</span><a href="http://matt.might.net/articles/how-to-peer-review/">how to peer review</a><span>&nbsp;and&nbsp;</span><a href="http://matt.might.net/articles/peer-fortress/">Peer Fortress: The Scientific Battlefield</a><span>&nbsp;(the last one provides examples NOT to follow)</span></li>
              <li><a href="http://www.pamitc.org/cvpr16/reviewer_guidelines.php">CVPR 2016 Reviewer’s Guidelines</a><span>.</span></li>
              <li><a href="http://mobilehci.acm.org/2015/download/ExcellenceInReviewsforHCICommunity.pdf" class="pdf-link">So You're a Program Committee Member Now: On Excellence in Reviews and Meta-Reviews and Championing Submitted Work That Has Merit</a></li>
            </ul>
            <h1>Score Breakdown</h1>
            <ul>
              <li><span>30% on Kaggle performance (score on the private test set).</span></li>
              <li><span>30% on report: is the proposed method clearly described? is it professionally written? does it include enough detail for a professional (ie. skilled graduate student) to re-implement the results? A good report should at least contain a brief introduction of what you did, a detailed description of your method, an experimental evaluation part that&nbsp;shows the experimental results and analysis of your results.&nbsp;</span></li>
              <li><span>20% on method: does it make sense? is it suited to the task? does it make use of the data in appropriate ways?</span></li>
              <li><span>10% on peer reviews from other classmates.</span></li>
              <li><span>10% on the quality of peer review: is it well-written and thoughtful? does it provide insight to the authors?</span></li>
            </ul>
          </div>


          <div>
            <h2>Data files: Download <a href="https://mjw-cornell-se3-public.s3.amazonaws.com/cs5785-final-spring2017-data.zip">here!</a></h2>
            <ul>
              <li><strong>images_train</strong>&nbsp;- 10,000 training images of size 224x224.</li>
              <li><strong>images_test</strong>&nbsp;- 2,000 test&nbsp;images of size 224x224.</li>
              <li><strong>tags_train</strong>&nbsp;- image tags correspond to training images. Each image have several tags indicating the human-labeled object categories appear in the image, in the form of "supercategory:category".</li>
              <li><strong>tags_test</strong>&nbsp;- image tags correspond to test&nbsp;images. Each image have several tags indicating the human-labeled object categories appear in the image, in the form of "supercategory:category".</li>
              <li><strong>features_train</strong>&nbsp;- features extracted from a pre-trained Residual Network (ResNet) on training set, including 1,000 dimensional feature from classification&nbsp;layer (fc1000) and 2,048 dimensional feature from final convolution layer (pool5). Each dimension of the fc1000 feature corresponds to a WordNet synset <a href="https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a">here</a>.</li>
              <li><strong>features_test</strong>&nbsp;- features extracted from the same&nbsp;Residual Network (ResNet) on test&nbsp;set, including 1,000 dimensional feature from classification&nbsp;layer&nbsp;(fc1000) and 2,048 dimensional feature from final convolution layer (pool5).</li>
              <li><strong>descriptions_train</strong>&nbsp;- image descriptions correspond to training images. Each image have 5 sentences for describing the image content.</li>
              <li><strong>descriptions_test</strong>&nbsp;-&nbsp;image descriptions for&nbsp;test images. Each image have 5&nbsp;sentences&nbsp;for describing the image content.&nbsp;<em><strong>Notice that one test description corresponds to one test image. The task you need to do is to return&nbsp;top 20 images in test set for each test description.</strong></em></li>
              <li><strong>sample_submission.csv</strong>&nbsp;- a sample submission file in the correct format.</li>
            </ul>

          </div>

        </div>
      </div>
    </div>

    <hr>

    <!-- Footer -->
    <footer>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
            <p class="copyright text-muted">Copyright &copy; Applied Machine Learning 2016<br/>Maintanence: Longqi: ly283@cornell.edu</p>
          </div>
        </div>
    </footer>

    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Custom Theme JavaScript -->
    <script src="js/clean-blog.min.js"></script>

  </body>

</html>
